# Edge Cloud Collaboration
This Readme explains how to execute Image classfication models
for edge-cloud collaboration. The algorithm splits the DNN automatically
to find the optimal split to minimize end-to-end latency. It also 
applies mixed precision quantization on the edge DNN and allocates 
the required bit-width.   

Following torchvision models have been tested so far: 

````
import torchvision.models as models
resnet18 = models.resnet18()
googlenet = models.googlenet()
mobilenet = models.mobilenet_v2()
resnext50_32x4d = models.resnext50_32x4d()
mnasnet = models.mnasnet1_0()
resnet50 = models.resnet50()
````
***Make sure to set the correct paths at different stages***. 
***Also, make sure to set complete paths and not relative paths in thes scripts***.
------
### Run Scripts -- Quick without detailed explanation

**1. Collect Statistics**
 
Bash scripts to run all classification models can be found in  - 
[classification_run_scripts/](classification_run_scripts/). 
***Set the paths first*** in [classification_run_scripts/as_0_set_path.sh](as_0_set_path.sh)
**Uncomment and set path for export DATASET**
````
$ cd classification_run_scripts
$ bash as_1_collect_stats.sh
````
This code will generate a log folder with graph statistics.  For example, 
````
$ MODELNAME=resnet18
$ ROOTFOLDER=~/distributed-inference/auto_split
$ mkdir $ROOTFOLDER/data
$ mv ${log folder} $ROOTFOLDER/data/$MODELNAME
````
For a list of all modelnames: check `APPS` in [classification_run_scripts/as_0_set_path.sh](as_0_set_path.sh)
This statistics are required to be generated only once. 

For internal product, auto-split profiles weights and activation statistics for sample inputs. 
The profiled weights and activations are utilized in `auto_split_v2`.  
By default, in this version of the code this feature turned off. To turn on this feature, 
Uncomment ``#dump output activation non quantized version`` from the file: ``libs/distiller-master/distiller
 /quantization/range_linear_bit_search.py``.  At present, it collects stats for each input and overrides the results
  making it slow to process. This feature will be provided in future. 

------------
**2. Auto-split**

`Timestamp_v1` corresponds to `function: auto_split_v1` in `as_2_auto_split.sh`.
`Timestamp_v1` use Auto-split with random activations and weights based on model activation 
and weight dimensions. 

`Timestamp_v2` corresponds to `function: auto_split_v2` in `as_2_auto_split.sh`.
`Timestamp_v2` uses sampled activations and weigths generated by the DNN. 
This requires the generation of per layer statistics in stage ``1. collect statistics``.
It is turned off by default. See previous section to turn it on.    

````
$ bash as_2_auto_split.sh
````
This script generates statistics in a time stamp folder path `$ROOTFOLDER/bit_search/resnet18_20210205-203940`

------------
**3--5. Generate latency estimate and quantization accuracies**

For the remaining steps `set Timestamp_v1 and/or Timestamp_v2` in [classification_run_scripts/as_0_set_path.sh
](as_0_set_path.sh), Depending on the selected configuration in `as_2_auto_split.sh`. 

For the given example in previous step in the script: [classification_run_scripts/as_0_set_path.sh
](as_0_set_path.sh)
````
$ declare -A Timestamp_v1=(['resnet18']='resnet18_20210205-203940')
````
Once, the Timestamp_v1 folder is set, run each script in the following order :- 
````
$ bash as_3_gen_latency.sh
$ bash as_4_run_quantization.sh
$ bash as_5_collect_accuracy.sh
````
* ``as_3_gen_latency.sh``: executes latency for various configurations in parallel.
* ``as_4_run_quantization.sh``: This script runs three steps 
    * a) collect latency stats, through function `collect_latency_v1`
    * b) generate quantization bits per layer in "*.yaml" file through function `set_yaml_v1`
    * c) Run LAPQ quantization: through, function ``run_quantization_v1 $DEVICEID``. This step is slow and takes time to
     execute various bit-width configurations generated by Auto-split and collect accuracy.
* ``as_5_collect_accuracy.sh``: Collects accuracy generated in the previous step.
----
**6. Run Neurosurgeon and DADS/QDMP**

Note: This step requires the graph statistics which were already generated in first
step ``1. Collect Statistics`` 

* Generate per layer latency stats on post processed graph. 
````
$ bash run_neurosurgeon_1.sh
# This code uses: 
$ $PYTHON tools/bit_search/neurosurgeon_1_gen.py --arch $i
````
* Collect Split points and latency stats for Neurosurgeon and DADS/QDMP  on post processed graph.
````
$ $PYTHON tools/bit_search/neurosurgeon_2_cycles.py
````


# Detailed Explanation 
A step by step guide is provided below for each step is explained below :-  



### Set Root directory
 The set of commands listed below is added in `run_model.sh`.
 Please note: the entire script is not supposed to be run togethher 
 as some steps include multi-threaded execution and is meant for refernce.
 
 
````
# export ROOTFOLDER=~/edge-cloud-collaboration/auto_split
$ export MODELNAME=resnet50
$ export PYTHONPATH=~/edge-cloud-collaboration/auto_split:~/edge-cloud-collaboration/libs/distiller-master
````
# Bit Search 

Run auto-split bit search algorithm, and generate bit configurations 
and latency mse trade-offs

## Step 1: Collect Quantization statistics 
* Generate Quant stats file. You will get the following stats 
file `data/stats/resnet50_quant_stats.yaml` as output. 
This step needs to be run only once. 
Comment out `model_activation_stats` from [resnet50_imagenet_post_train.yaml](data/resnet50/resnet50_imagenet_post_train.yaml).
Then run the command in Step 2 to generate stats. Uncomment  `model_activation_stats` once, the stats file is generated.

## Step 2: Apply DNN graph operator fusion. 


In this step, we take as input [resnet50_quant_stats.yaml](data/stats/resnet50_quant_stats.yaml).
Prepare a [resnet50_imagenet_post_train.yaml](data/resnet50/resnet50_imagenet_post_train.yaml) file 
and execute [compress_classifier.py](tools/bit_search/compression/compress_classifier.py) script to generate quant stats file and layer ranks post DNN graph operator fusion. 


````
$ python tools/bit_search/compression/compress_classifier.py  --arch $MODELNAME \
--pretrained --evaluate --quantize-eval --qe-lapq \
--qe-config-file data/resnet50/resnet50_imagenet_post_train.yaml
````
Output: 

| Generated Files                      | Remarks                                                                     |
|--------------------------------------|-----------------------------------------------------------------------------|
| acts_quantization_stats.yaml         | activation statistics pre operator fusion                                   |
| quant_stats_after_prepare_model.yaml | activation stats post operator fusion                                       |
| layer_ranks.csv                      | topological order of DNN graph nodes                                        |
| post_prepare_model_df.csv            | layer stats dataFrame used for latency calculation and auto-split algorithm |
| weights_data.pkl                     | weight data for dummy input in pickle file                                  |
| weight_stats.pkl                     | layerwise weight statistics in pickle file                                  |
| out_act/`$layer_names`/act_data.pkl   | activation data for dummy input                                             |
| out_act/`$layer_names`/act_stats.pkl  | activation statistics for dummy input                                       |


Copy all files 
````
$ cd latest_log_dir
$ cp -r * $ROOTFOLDER/data/$MODELNAME/.
$ cd $ROOTFOLDER
````


## Step 3: Post process 
The file `post_prepare_model_df.csv` contains basic layer stats. 
We need to add more stats such as layer ranks, min, max, mean, std.
````
$ python tools/bit_search/add_rank_min_max_stats.py --arch $MODELNAME
````
* Input: `tools/auto_split/data/resnet50/post_prepare_model_df.csv`
* Output: `tools/generated/hw_simulator/post_process2/resnet50.csv`

## Step 4: Run Auto-split algorithm
The auto_split.py searches for optimal split and generates mse stats 
and bit allocations. 
* Input: `generated/hw_simulator/post_process2/$MODELNAME.csv`
* Output: `generated/bit_search/${MODELNAME}_timestamp/(latency_mse_stats.csv and several bit confiurations)` 
say `TIMESTAMPFOLDER=resnet50_20200521-174448`
(bit configuration files, latency_mse_stats.csv, net_df.csv)

* Auto_split-v1: Generates random activations and weights based on the layer shapes.
* Auto-split-v2: Reads sample activations and weights (.pkl files) generated from the DNN in 
[Step 2](#step-2-apply-dnn-graph-operator-fusion)

````
# For running Auto-split-v1
$ python tools/bit_search/auto_split.py --arch $MODELNAME
# For running Auto-split-v2 
$ python tools/bit_search/auto_split.py --arch $MODELNAME --logdir data/$MODELNAME
````
Look for the generated folder in `$ROOTFOLDER/generated/bit_search`
````
$ export TIMESTAMPFOLDER=resnet50_20200521-174448
````
The second script takes input from  stats generated by 
auto_split.py,  and generates latency estimates based on the simulator. 
The simulator is based on 
[ARM-software/SCALE-Sim](https://github.com/ARM-software/SCALE-Sim).  
* Input: `generated/bit_search/$TIMESTAMPFOLDER`
* Output: `generated/latency/$TIMESTAMPFOLDER/(latency of each bit configuration)`

Currently, the scipt executes selected configurations only 
but it can be modified to get estimate of all configurations.
For example, the parameter `lower_mse_than_bit=2` in 
the `__main__` function of [model_latency_2_gen.py](tools/bit_search/model_latency_2_gen.py)
will generate latency for the configurations whose MSE is less than uniform 2-bit MSE. 
`lower_mse_than_bit=None` generates all configurations.

````
$ python tools/bit_search/model_latency_2_gen.py -n $TIMESTAMPFOLDER -t 16
````
The third script takes as input 
* latency estimates generated in the previous step, 
* `latency_mse_stats.csv` generated by `auto_split.py` 

and and adds latency values to generate the following output: 
* Output: `generated/latency/$TIMESTAMPFOLDER/(latency_mse_stats.csv & latency_summary.csv`

````
$ python tools/bit_search/model_latency_3_collect.py -n $TIMESTAMPFOLDER 
````


# Run Quantization

### Step 1: Generate yaml config from bit search
````
$ python tools/run_quantization/set_yaml_config_lapq.py -n $TIMESTAMPFOLDER --arch $MODELNAME
````
This script takes layer wise bit configurations from 
* Input: `generated/bit_search/$TIMESTAMPFOLDER` and 

generates yaml configurations for distiller post train quantization in
* Output: `generated/run_quantization/$TIMESTAMPFOLDER`

### Step 2: Run distiller quantization 
````
export DATASET=<path to imagenet dataset>
$ python tools/run_quantization/run_distiller.py -n $TIMESTAMPFOLDER --arch $MODELNAME $DATASET
````
This script runs various yaml configurations with distiller post train 
quantization techniques. 
* Input: `generated/run_quantization/resnet50_20200301-234408`

The results are generated in 
* Output `generated/quant_results/$TIMESTAMPFOLDER/logs`

### Step 3: Accumulate accuracy stats

````
$ python tools/run_quantization/get_accuracy.py -n $TIMESTAMPFOLDER
````
This script accumulates the results of accuracies from previous runs 
and collects 'config,Top1,Top5' accuracies from generated logs in previous step.
The results are stored in 
* Output : `generated/quant_results/accuracy_$TIMESTAMPFOLDER.csv`
